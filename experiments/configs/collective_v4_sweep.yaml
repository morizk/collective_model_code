# Wandb Sweep Configuration - COLLECTIVE MODEL V4
# Latest optimized configuration with model-level dropout
# Updated: Based on latest sweep insights and findings
#
# Key features:
# - Model-level dropout (NEW - ensemble regularization)
# - Batch size: [256, 512] (optimized)
# - Uniform c_expert distribution (continuous exploration)
# - Focused n_total range: [12, 14]
# - Wider learning rate range: [0.0001, 0.02]

program: train.py
method: bayes  # Bayesian optimization (smart search)

metric:
  goal: maximize
  name: param_efficiency/test_accuracy_per_log10_params  # Optimize for parameter efficiency (log-scaled)

parameters:
  # =============================================================================
  # MODEL TYPE - COLLECTIVE ONLY
  # =============================================================================
  model_type:
    value: collective  # ONLY collective model

  # =============================================================================
  # FIXED PARAMETERS (Fashion-MNIST)
  # =============================================================================
  dataset:
    value: fashion_mnist  # Much harder than MNIST (88-92% vs 98%+)
  input_dim:
    value: 784
  num_classes:
    value: 10
  device:
    value: cuda
  seed:
    value: 42

  # =============================================================================
  # TRAINING PARAMETERS (OPTIMIZED)
  # =============================================================================
  epochs:
    value: 100
  
  batch_size:
    values: [256, 512]  # Optimized: NEW results show 256-512 perform better
  
  eval_batch_size:
    values: [1024, 2048]  # Larger batch for validation/test
  
  learning_rate:
    distribution: log_uniform_values
    min: 0.0001
    max: 0.02  # Wider range for exploration
  
  optimizer:
    value: adamw  # Best optimizer based on findings
  
  weight_decay:
    values: [0.0, 0.0001, 0.001]
  
  use_augmentation:
    value: true  # Fixed: augmentation helps

  # =============================================================================
  # COLLECTIVE ARCHITECTURE PARAMETERS
  # =============================================================================
  
  # Number of models
  n_total:
    values: [12, 14]  # Focused range based on findings
  
  # Expert ratio (scales with n_total)
  expert_ratio:
    distribution: uniform
    min: 0.1
    max: 0.35  # Lower is better based on sweep insights
  
  # Expert architecture
  expert_hidden:
    values:
      - [512, 256]      # Standard
      - [768, 384]      # Larger
      - [384, 192]      # Smaller
      - [512, 256, 128] # Deeper
  
  expert_output:
    values: [64, 128, 256]
  
  # Analyst architecture
  analyst_hidden:
    values:
      - [256, 128]      # Standard
      - [384, 192]      # Larger
      - [192, 96]       # Smaller
      - [256, 128, 64]  # Deeper
  
  analyst_output:
    values: [32, 64, 128]
  
  # Compression ratios
  c_expert:
    distribution: uniform
    min: 0.1
    max: 0.25  # Uniform distribution for continuous exploration
  
  c_collective:
    values: [0.1, 0.25, 0.5]  # Collective encoder compression
  
  # Collective architecture
  collective_version:
    value: simple_mlp  # Phase 1: Only simple_mlp
  
  collective_hidden_scale:
    values: [0.5, 1.0, 1.5]

  # =============================================================================
  # DIVERSITY PARAMETERS (OPTIMIZED)
  # =============================================================================
  
  use_expert_diversity:
    value: true  # Fixed: helps performance
  
  use_analyst_diversity:
    value: true  # Fixed: important finding
  
  diversity_lambda:
    values: [0.0, 0.001, 0.01, 0.05]  # 0.0 = off
  
  diversity_temperature:
    values: [0.1, 1.0, 5.0]  # Negative correlation found - helps performance
  
  vary_expert_architectures:
    values: [true, false]
  
  vary_analyst_architectures:
    value: true  # Fixed: important finding

  # =============================================================================
  # MODEL-LEVEL DROPOUT PARAMETERS (NEW - ensemble regularization)
  # =============================================================================
  
  use_drop_models:
    values: [true, false]  # Enable/disable model-level dropout
  
  drop_models_expert_rate:
    distribution: uniform
    min: 0.0
    max: 0.2  # Dropout rate for experts (0.0-0.2, each expert has this prob of being dropped)
  
  drop_models_analyst_rate:
    distribution: uniform
    min: 0.0
    max: 0.2  # Dropout rate for analysts (0.0-0.2, each analyst has this prob of being dropped)

  # =============================================================================
  # DERIVED PARAMETERS (DO NOT SET - computed automatically)
  # =============================================================================
  # These are computed by prepare_config():
  # - n_experts (from n_total * expert_ratio)
  # - n_analysts (from n_total - n_experts)
  # - expert_encoder_output_dim
  # - analyst_input_dim
  # - collective_input_dim

# =============================================================================
# EARLY TERMINATION (DISABLED)
# =============================================================================
# Early termination removed to ensure all runs complete full 100 epochs
# This provides complete data for analysis, even for underperforming configs
# Note: Will take longer but gives more comprehensive results

# =============================================================================
# SWEEP INSIGHTS APPLIED (v4)
# =============================================================================
# Based on v1 + v2 + v3 sweep results:
# 1. adamw > adam (fixed)
# 2. use_expert_diversity=true helps (fixed)
# 3. use_analyst_diversity=true IMPORTANT (fixed)
# 4. vary_analyst_architectures=true IMPORTANT (fixed)
# 5. batch_size=[256, 512] NEW results show better than 128 (UPDATED)
# 6. learning_rate: Expanded to 0.02 max (wider range for exploration)
# 7. c_expert: Uniform distribution [0.1, 0.25] (continuous exploration)
# 8. n_total: [12, 14] (focused range)
# 9. use_augmentation=true (fixed)
# 10. Lower expert_ratio (0.1-0.35) performs better than higher ratios
# 11. diversity_temperature shows negative correlation (helps, keep range)
# 12. Added model-level dropout parameters (NEW feature - ensemble regularization)
# 13. Only log-scaled efficiency metrics (reduce clutter)
# 14. Fixed loss efficiency calculation (lower is better, not inverse)

