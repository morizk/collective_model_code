# Wandb Sweep Configuration - COLLECTIVE MODEL ONLY
# Optimized for PARAMETER EFFICIENCY (accuracy per 100K parameters)
# Uses same config as accuracy sweep but optimizes for efficiency instead
# Bayesian optimizer will learn from BOTH accuracy-focused AND efficiency-focused runs!

program: train.py
method: bayes  # Bayesian optimization (uses ALL previous runs to guide search)

metric:
  goal: maximize
  name: final/param_efficiency  # Optimize for accuracy per 100K parameters (not raw accuracy!)

parameters:
  # =============================================================================
  # MODEL TYPE - COLLECTIVE ONLY
  # =============================================================================
  model_type:
    value: collective  # ONLY collective model

  # =============================================================================
  # FIXED PARAMETERS (Fashion-MNIST)
  # =============================================================================
  dataset:
    value: fashion_mnist  # Much harder than MNIST (88-92% vs 98%+)
  input_dim:
    value: 784
  num_classes:
    value: 10
  device:
    value: cuda
  seed:
    value: 42  # Fixed seed for reproducibility

  # =============================================================================
  # TRAINING PARAMETERS (SHARED - tune these)
  # =============================================================================
  epochs:
    value: 200  # Fixed for fair comparison
  
  batch_size:
    values: [256, 512]  # Increased for smoother validation/test curves
  eval_batch_size:
    values: [512, 1024]  # Larger batch for validation/test (smoother metrics)
  
  learning_rate:
    distribution: log_uniform_values
    min: 0.0001
    max: 0.01
  
  optimizer:
    values: [adam, adamw]
  
  weight_decay:
    values: [0.0, 0.0001, 0.001]
  
  use_augmentation:
    value: true  # Default: Use data augmentation

  # =============================================================================
  # COLLECTIVE ARCHITECTURE PARAMETERS (CORE - tune these!)
  # =============================================================================
  
  # Number of models
  n_total:
    values: [6, 8, 10, 12]  # Total experts + analysts
  
  expert_ratio:
    distribution: uniform
    min: 0.1
    max: 0.4  # 10-40% of models are experts
  
  # Expert architecture (larger than analysts)
  expert_hidden:
    values:
      - [256, 128]  # Smaller experts (more parameter-efficient)
      - [384, 192]
      - [512, 256]
      - [768, 384]  # Larger experts (higher capacity)
  
  expert_output:
    values: [64, 128, 256]  # Output dimensions
  
  # Analyst architecture (smaller than experts)
  analyst_hidden:
    values:
      - [128, 64]  # Small analysts (parameter-efficient)
      - [256, 128]
      - [384, 192]  # Larger analysts
  
  analyst_output:
    values: [32, 64, 128]  # Output dimensions
  
  # Compression ratios
  c_expert:
    values: [0.1, 0.25, 0.5]  # Compress expert outputs (10%, 25%, 50%)
  
  c_collective:
    values: [0.1, 0.25, 0.5]  # Compress collective input (for v2 only)
  
  # Collective layer type
  collective_version:
    values: [simple_mlp]  # Phase 1: only simple MLP (encoder_head not implemented yet)
  
  # Collective layer size
  collective_hidden_scale:
    values: [0.5, 1.0, 1.5]  # Scale relative to analyst output size
  
  # =============================================================================
  # DIVERSITY PARAMETERS
  # =============================================================================
  use_expert_diversity:
    values: [true, false]
  
  use_analyst_diversity:
    values: [true, false]
  
  diversity_lambda:
    values: [0.0, 0.001, 0.01, 0.05]  # Weight for diversity loss
  
  diversity_temperature:
    values: [0.1, 1.0, 5.0]  # Temperature for diversity (higher = less aggressive)
  
  # Architectural diversity (vary sizes within group)
  vary_expert_architectures:
    values: [true, false]
  
  vary_analyst_architectures:
    values: [true, false]

# Early termination (Hyperband)
early_terminate:
  type: hyperband
  min_iter: 20  # Minimum epochs before early stopping
  max_iter: 200  # Maximum epochs
  s: 2  # Brackets
  eta: 3  # Elimination rate

