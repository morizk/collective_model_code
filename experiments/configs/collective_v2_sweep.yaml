# Wandb Sweep Configuration - COLLECTIVE MODEL V2
# Optimized based on sweep insights from v1
# Changes:
# - adamw only (adam removed)
# - use_expert_diversity=true (default)
# - Reduced learning_rate range (0.0001-0.006)
# - Increased batch_size (512, 1024)
# - Use n_experts directly (1-4) instead of expert_ratio
# - 125 epochs
# - Only log-scaled efficiency metrics

program: train.py
method: bayes  # Bayesian optimization (smart search)

metric:
  goal: maximize
  name: final/test_accuracy

parameters:
  # =============================================================================
  # MODEL TYPE - COLLECTIVE ONLY
  # =============================================================================
  model_type:
    value: collective  # ONLY collective model

  # =============================================================================
  # FIXED PARAMETERS (Fashion-MNIST)
  # =============================================================================
  dataset:
    value: fashion_mnist  # Much harder than MNIST (88-92% vs 98%+)
  input_dim:
    value: 784
  num_classes:
    value: 10
  device:
    value: cuda
  seed:
    value: 42

  # =============================================================================
  # TRAINING PARAMETERS (OPTIMIZED BASED ON SWEEP INSIGHTS)
  # =============================================================================
  epochs:
    value: 125  # Reduced for faster iteration
  
  batch_size:
    values: [128, 256]  # Reverted - wandb shows 128 performs better
  eval_batch_size:
    values: [512, 1024]  # Larger batch for validation/test (smoother metrics)
  
  learning_rate:
    distribution: log_uniform_values
    min: 0.0001
    max: 0.006  # Reduced from 0.01 based on sweep insights
  
  optimizer:
    value: adamw  # adamw is better, make it consistent
  
  weight_decay:
    values: [0.0, 0.0001, 0.001]
  
  use_augmentation:
    value: true  # Default: Use data augmentation

  # =============================================================================
  # COLLECTIVE ARCHITECTURE PARAMETERS
  # =============================================================================
  
  # Number of models
  n_total:
    values: [ 10, 12 ,14, 16]  # Total experts + analysts
  
  # Expert ratio (scales with n_total - more adaptable than fixed n_experts)
  expert_ratio:
    distribution: uniform
    min: 0.1
    max: 0.35  # Reduced from 0.4 - lower is better based on sweep insights
  
  # Expert architecture
  expert_hidden:
    values:
      - [512, 256]      # Standard
      - [768, 384]      # Larger
      - [384, 192]      # Smaller
      - [512, 256, 128] # Deeper
  
  expert_output:
    values: [64, 128, 256]
  
  # Analyst architecture
  analyst_hidden:
    values:
      - [256, 128]      # Standard
      - [384, 192]      # Larger
      - [192, 96]       # Smaller
      - [256, 128, 64]  # Deeper
  
  analyst_output:
    values: [32, 64, 128]
  
  # Compression ratios
  c_expert:
    values: [0.1, 0.25, 0.5]  # Expert encoder compression
  
  c_collective:
    values: [0.1, 0.25, 0.5]  # Collective encoder compression
  
  # Collective architecture
  collective_version:
    value: simple_mlp  # Phase 1: Only simple_mlp
  
  collective_hidden_scale:
    values: [0.5, 1.0, 1.5]

  # =============================================================================
  # DIVERSITY PARAMETERS (OPTIMIZED)
  # =============================================================================
  
  use_expert_diversity:
    value: true  # Default true - helps performance
  
  use_analyst_diversity:
    values: [true, false]
  
  diversity_lambda:
    values: [0.0, 0.001, 0.01, 0.05]  # 0.0 = off
  
  diversity_temperature:
    values: [0.1, 1.0, 5.0]  # Negative correlation found - helps performance
  
  vary_expert_architectures:
    values: [true, false]
  
  vary_analyst_architectures:
    values: [true, false]

  # =============================================================================
  # DERIVED PARAMETERS (DO NOT SET - computed automatically)
  # =============================================================================
  # These are computed by prepare_config():
  # - n_experts (from n_total * expert_ratio)
  # - n_analysts (from n_total - n_experts)
  # - expert_encoder_output_dim
  # - analyst_input_dim
  # - collective_input_dim

# =============================================================================
# EARLY TERMINATION (Hyperband - stop bad runs early)
# =============================================================================
early_terminate:
  type: hyperband
  min_iter: 20      # All runs get at least 20 epochs
  max_iter: 125     # Best runs go to 125 epochs
  s: 2
  eta: 3            # Keep top 1/3 at each stage

# =============================================================================
# SWEEP INSIGHTS APPLIED
# =============================================================================
# Based on v1 sweep results:
# 1. adamw > adam (now fixed)
# 2. use_expert_diversity=true helps (now default)
# 3. batch_size=128 performs better (reverted from 512-1024 based on wandb results)
# 4. Lower learning_rate range (0.0001-0.006) better
# 5. Lower expert_ratio (0.1-0.35) performs better than higher ratios
# 6. diversity_temperature shows negative correlation (helps, keep range)
# 7. Only log-scaled efficiency metrics (reduce clutter)
# 8. Fixed loss efficiency calculation (lower is better, not inverse)

