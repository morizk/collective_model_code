# Wandb Sweep Configuration - COLLECTIVE MODEL V3
# Optimized based on sweep insights from v1 and v2
# Changes from v2:
# - use_analyst_diversity=true (FIXED - important finding)
# - vary_analyst_architectures=true (FIXED - important finding)
# - Increased learning_rate max: 0.006 → 0.01 (expanded search space)
# - Decreased c_expert mean: [0.1,0.25,0.5] → [0.1,0.15,0.2,0.25] (lower values preferred)
# - 100 epochs
# Previous fixes:
# - adamw only (adam removed)
# - use_expert_diversity=true (default)
# - batch_size=128 performs better (reverted from 512-1024)
# - expert_ratio used (min 2 experts enforced automatically by code)

program: train.py
method: bayes  # Bayesian optimization (smart search)

metric:
  goal: maximize
  name: param_efficiency/test_accuracy_per_log10_params  # Optimize for parameter efficiency (log-scaled)

parameters:
  # =============================================================================
  # MODEL TYPE - COLLECTIVE ONLY
  # =============================================================================
  model_type:
    value: collective  # ONLY collective model

  # =============================================================================
  # FIXED PARAMETERS (Fashion-MNIST)
  # =============================================================================
  dataset:
    value: fashion_mnist  # Much harder than MNIST (88-92% vs 98%+)
  input_dim:
    value: 784
  num_classes:
    value: 10
  device:
    value: cuda
  seed:
    value: 42

  # =============================================================================
  # TRAINING PARAMETERS (OPTIMIZED BASED ON SWEEP INSIGHTS)
  # =============================================================================
  epochs:
    value: 100
  
  batch_size:
    values: [128, 256]  # Reverted - wandb shows 128 performs better
  eval_batch_size:
    values: [1024, 2048]  # Larger batch for validation/test
  
  learning_rate:
    distribution: log_uniform_values
    min: 0.0001
    max: 0.01  # Increased search space based on new findings
  
  optimizer:
    value: adamw  # adamw is better, make it consistent
  
  weight_decay:
    values: [0.0, 0.0001, 0.001]
  
  use_augmentation:
    value: true  # Default: Use data augmentation

  # =============================================================================
  # COLLECTIVE ARCHITECTURE PARAMETERS
  # =============================================================================
  
  # Number of models
  n_total:
    values: [ 10, 12 ,14, 16]  # Total experts + analysts
  
  # Expert ratio (scales with n_total - more adaptable than fixed n_experts)
  expert_ratio:
    distribution: uniform
    min: 0.1
    max: 0.35  # Reduced from 0.4 - lower is better based on sweep insights
  
  # Expert architecture
  expert_hidden:
    values:
      - [512, 256]      # Standard
      - [768, 384]      # Larger
      - [384, 192]      # Smaller
      - [512, 256, 128] # Deeper
  
  expert_output:
    values: [64, 128, 256]
  
  # Analyst architecture
  analyst_hidden:
    values:
      - [256, 128]      # Standard
      - [384, 192]      # Larger
      - [192, 96]       # Smaller
      - [256, 128, 64]  # Deeper
  
  analyst_output:
    values: [32, 64, 128]
  
  # Compression ratios
  c_expert:
    values: [0.1, 0.15, 0.2, 0.25]  # Expert encoder compression (decreased mean - lower values preferred)
  
  c_collective:
    values: [0.1, 0.25, 0.5]  # Collective encoder compression
  
  # Collective architecture
  collective_version:
    value: simple_mlp  # Phase 1: Only simple_mlp
  
  collective_hidden_scale:
    values: [0.5, 1.0, 1.5]

  # =============================================================================
  # DIVERSITY PARAMETERS (OPTIMIZED)
  # =============================================================================
  
  use_expert_diversity:
    value: true  # Default true - helps performance
  
  use_analyst_diversity:
    value: true  # Fixed to true - important based on new findings
  
  diversity_lambda:
    values: [0.0, 0.001, 0.01, 0.05]  # 0.0 = off
  
  diversity_temperature:
    values: [0.1, 1.0, 5.0]  # Negative correlation found - helps performance
  
  vary_expert_architectures:
    values: [true, false]
  
  vary_analyst_architectures:
    value: true  # Fixed to true - important based on new findings

  # =============================================================================
  # DERIVED PARAMETERS (DO NOT SET - computed automatically)
  # =============================================================================
  # These are computed by prepare_config():
  # - n_experts (from n_total * expert_ratio)
  # - n_analysts (from n_total - n_experts)
  # - expert_encoder_output_dim
  # - analyst_input_dim
  # - collective_input_dim

# =============================================================================
# EARLY TERMINATION (Hyperband - stop bad runs early)
# =============================================================================
early_terminate:
  type: hyperband
  min_iter: 20      # All runs get at least 20 epochs
  max_iter: 100     # Best runs go to 100 epochs
  s: 2
  eta: 3            # Keep top 1/3 at each stage

# =============================================================================
# SWEEP INSIGHTS APPLIED
# =============================================================================
# Based on v1 + v2 sweep results:
# 1. adamw > adam (now fixed)
# 2. use_expert_diversity=true helps (now fixed)
# 3. use_analyst_diversity=true IMPORTANT (now fixed - NEW FINDING)
# 4. vary_analyst_architectures=true IMPORTANT (now fixed - NEW FINDING)
# 5. batch_size=128 performs better (reverted from 512-1024 based on wandb results)
# 6. learning_rate: Expanded to 0.01 max (test higher rates - NEW)
# 7. c_expert: Decreased mean [0.1,0.15,0.2,0.25] (lower values preferred - NEW)
# 8. Lower expert_ratio (0.1-0.35) performs better than higher ratios
# 9. diversity_temperature shows negative correlation (helps, keep range)
# 10. Only log-scaled efficiency metrics (reduce clutter)
# 11. Fixed loss efficiency calculation (lower is better, not inverse)

